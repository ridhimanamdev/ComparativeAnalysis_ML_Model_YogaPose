{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":4161775,"sourceType":"datasetVersion","datasetId":2456516}],"dockerImageVersionId":30236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import Data Science Libraries\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nimport itertools\nimport random\n\n# Import visualization libraries\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport cv2\n\n# Tensorflow Libraries\nfrom tensorflow import keras\nfrom tensorflow.keras import layers,models\nfrom keras_preprocessing.image import ImageDataGenerator\nfrom keras.layers import Dense, Dropout\nfrom tensorflow.keras.callbacks import Callback, EarlyStopping,ModelCheckpoint\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.layers.experimental import preprocessing\n\n# System libraries\nfrom pathlib import Path\nimport os.path\n\n# Metrics\nfrom sklearn.metrics import classification_report, confusion_matrix","metadata":{"id":"WVPt9AGkKPIw","execution":{"iopub.status.busy":"2024-01-07T05:08:58.611133Z","iopub.execute_input":"2024-01-07T05:08:58.612326Z","iopub.status.idle":"2024-01-07T05:08:58.621743Z","shell.execute_reply.started":"2024-01-07T05:08:58.612268Z","shell.execute_reply":"2024-01-07T05:08:58.620648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ü§ôCreate helper functions","metadata":{"id":"EzK7Ms9KKTZD"}},{"cell_type":"code","source":"!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n\n# Import series of helper functions for our notebook\nfrom helper_functions import create_tensorboard_callback, plot_loss_curves, unzip_data, compare_historys, walk_through_dir, pred_and_plot","metadata":{"id":"BJreWyjXKUtD","outputId":"84f9bb7e-34cb-4a09-abf9-f77d964756f0","execution":{"iopub.status.busy":"2024-01-07T05:08:58.624065Z","iopub.execute_input":"2024-01-07T05:08:58.624560Z","iopub.status.idle":"2024-01-07T05:08:59.926901Z","shell.execute_reply.started":"2024-01-07T05:08:58.624492Z","shell.execute_reply":"2024-01-07T05:08:59.925586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üì•Load and transform data","metadata":{"id":"dr5w2Ea0KVwA"}},{"cell_type":"code","source":"BATCH_SIZE = 32\nIMAGE_SIZE = (224, 224)","metadata":{"id":"r-O66wuVKZqC","execution":{"iopub.status.busy":"2024-01-07T05:08:59.929010Z","iopub.execute_input":"2024-01-07T05:08:59.929534Z","iopub.status.idle":"2024-01-07T05:08:59.935766Z","shell.execute_reply.started":"2024-01-07T05:08:59.929467Z","shell.execute_reply":"2024-01-07T05:08:59.934549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Walk through each directory\ndataset = \"../input/yoga-posture-dataset\"\nwalk_through_dir(dataset)","metadata":{"id":"YZnj-MqoKfCS","outputId":"307b07c9-866b-47d4-dd51-558a9c0bf1ce","execution":{"iopub.status.busy":"2024-01-07T05:08:59.938962Z","iopub.execute_input":"2024-01-07T05:08:59.939491Z","iopub.status.idle":"2024-01-07T05:08:59.985672Z","shell.execute_reply.started":"2024-01-07T05:08:59.939455Z","shell.execute_reply":"2024-01-07T05:08:59.984495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üìÖPlacing data into a Dataframe\nThe first column `filepaths` contains the file path location of each individual images. The second column `labels`, on the other hand, contains the class label of the corresponding image from the file path","metadata":{"id":"DMtm7GaNKgGi"}},{"cell_type":"code","source":"image_dir = Path(dataset)\n\n# Get filepaths and labels\nfilepaths = list(image_dir.glob(r'**/*.JPG')) + list(image_dir.glob(r'**/*.jpg')) + list(image_dir.glob(r'**/*.png')) + list(image_dir.glob(r'**/*.png'))\n\nlabels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1], filepaths))\n\nfilepaths = pd.Series(filepaths, name='Filepath').astype(str)\nlabels = pd.Series(labels, name='Label')\n\n# Concatenate filepaths and labels\nimage_df = pd.concat([filepaths, labels], axis=1)","metadata":{"id":"WyM0uiN_Kh5S","execution":{"iopub.status.busy":"2024-01-07T05:08:59.987224Z","iopub.execute_input":"2024-01-07T05:08:59.987605Z","iopub.status.idle":"2024-01-07T05:09:00.227522Z","shell.execute_reply.started":"2024-01-07T05:08:59.987569Z","shell.execute_reply":"2024-01-07T05:09:00.226592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_df","metadata":{"id":"O5WxN84IKjOF","outputId":"453aa160-9ede-4603-bb46-3fc2bee3aefd","execution":{"iopub.status.busy":"2024-01-07T05:09:00.228752Z","iopub.execute_input":"2024-01-07T05:09:00.229101Z","iopub.status.idle":"2024-01-07T05:09:00.244538Z","shell.execute_reply.started":"2024-01-07T05:09:00.229067Z","shell.execute_reply":"2024-01-07T05:09:00.243252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üî≠Visualizing images from the dataset","metadata":{"id":"ElxqJmMiKk4_"}},{"cell_type":"code","source":"# Display 9 picture of the dataset with their labels\nrandom_index = np.random.randint(0, len(image_df), 16)\nfig, axes = plt.subplots(nrows=4, ncols=4, figsize=(10, 10),\n                        subplot_kw={'xticks': [], 'yticks': []})\n\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(plt.imread(image_df.Filepath[random_index[i]]))\n    ax.set_title(image_df.Label[random_index[i]])\nplt.tight_layout()\nplt.show()","metadata":{"id":"vRzsNtLiKmoq","outputId":"455bd7e9-47de-4601-d5bd-f14b70104741","execution":{"iopub.status.busy":"2024-01-07T05:09:00.246266Z","iopub.execute_input":"2024-01-07T05:09:00.246741Z","iopub.status.idle":"2024-01-07T05:09:01.792931Z","shell.execute_reply.started":"2024-01-07T05:09:00.246705Z","shell.execute_reply":"2024-01-07T05:09:01.791894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üßÆComputing Error Rate Analysis","metadata":{}},{"cell_type":"code","source":"def compute_ela_cv(path, quality):\n    temp_filename = 'temp_file_name.jpeg'\n    SCALE = 15\n    orig_img = cv2.imread(path)\n    orig_img = cv2.cvtColor(orig_img, cv2.COLOR_BGR2RGB)\n    \n    cv2.imwrite(temp_filename, orig_img, [cv2.IMWRITE_JPEG_QUALITY, quality])\n\n    # read compressed image\n    compressed_img = cv2.imread(temp_filename)\n\n    # get absolute difference between img1 and img2 and multiply by scale\n    diff = SCALE * cv2.absdiff(orig_img, compressed_img)\n    return diff\n\n\ndef convert_to_ela_image(path, quality):\n    temp_filename = 'temp_file_name.jpeg'\n    ela_filename = 'temp_ela.png'\n    image = Image.open(path).convert('RGB')\n    image.save(temp_filename, 'JPEG', quality = quality)\n    temp_image = Image.open(temp_filename)\n\n    ela_image = ImageChops.difference(image, temp_image)\n\n    extrema = ela_image.getextrema()\n    max_diff = max([ex[1] for ex in extrema])\n    if max_diff == 0:\n        max_diff = 1\n\n    scale = 255.0 / max_diff\n    ela_image = ImageEnhance.Brightness(ela_image).enhance(scale)\n    \n    return ela_image\n\n\ndef random_sample(path, extension=None):\n    if extension:\n        items = Path(path).glob(f'*.{extension}')\n    else:\n        items = Path(path).glob(f'*')\n        \n    items = list(items)\n        \n    p = random.choice(items)\n    return p.as_posix()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-07T05:09:01.794488Z","iopub.execute_input":"2024-01-07T05:09:01.794921Z","iopub.status.idle":"2024-01-07T05:09:01.811321Z","shell.execute_reply.started":"2024-01-07T05:09:01.794874Z","shell.execute_reply":"2024-01-07T05:09:01.809857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# View random sample from the dataset\np = random_sample('../input/yoga-posture-dataset/Adho Mukha Svanasana')\norig = cv2.imread(p)\norig = cv2.cvtColor(orig, cv2.COLOR_BGR2RGB) / 255.0\ninit_val = 100\ncolumns = 3\nrows = 3\n\nfig=plt.figure(figsize=(15, 10))\nfor i in range(1, columns*rows +1):\n    quality=init_val - (i-1) * 8\n    img = compute_ela_cv(path=p, quality=quality)\n    if i == 1:\n        img = orig.copy()\n    ax = fig.add_subplot(rows, columns, i) \n    ax.title.set_text(f'q: {quality}')\n    plt.imshow(img)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-07T05:09:01.815444Z","iopub.execute_input":"2024-01-07T05:09:01.815962Z","iopub.status.idle":"2024-01-07T05:09:03.540472Z","shell.execute_reply.started":"2024-01-07T05:09:01.815901Z","shell.execute_reply":"2024-01-07T05:09:03.539410Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üìùData Preprocessing\n<p style=\"font-size:15px; font-family:verdana; line-height: 1.7em\">The data will be split into three different categories: Training, Validation and Testing. The training data will be used to train the deep learning CNN model and its parameters will be fine tuned with the validation data. Finally, the performance of the data will be evaluated using the test data(data the model has not previously seen).</p>","metadata":{"id":"tfODtOOZKnqj"}},{"cell_type":"code","source":"# Separate in train and test data\ntrain_df, test_df = train_test_split(image_df, test_size=0.2, shuffle=True, random_state=1)","metadata":{"id":"rrWFc1rLK9fC","execution":{"iopub.status.busy":"2024-01-07T05:09:03.541909Z","iopub.execute_input":"2024-01-07T05:09:03.542277Z","iopub.status.idle":"2024-01-07T05:09:03.551646Z","shell.execute_reply.started":"2024-01-07T05:09:03.542242Z","shell.execute_reply":"2024-01-07T05:09:03.550405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_generator = ImageDataGenerator(\n    preprocessing_function=tf.keras.applications.mobilenet_v3.preprocess_input,\n    validation_split=0.2\n)\n\ntest_generator = ImageDataGenerator(\n    preprocessing_function=tf.keras.applications.mobilenet_v3.preprocess_input\n)","metadata":{"id":"ZatHTudzK_KX","execution":{"iopub.status.busy":"2024-01-07T05:09:03.553325Z","iopub.execute_input":"2024-01-07T05:09:03.553733Z","iopub.status.idle":"2024-01-07T05:09:03.560292Z","shell.execute_reply.started":"2024-01-07T05:09:03.553686Z","shell.execute_reply":"2024-01-07T05:09:03.559064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the data into three categories.\ntrain_images = train_generator.flow_from_dataframe(\n    dataframe=train_df,\n    x_col='Filepath',\n    y_col='Label',\n    target_size=(224, 224),\n    color_mode='rgb',\n    class_mode='categorical',\n    batch_size=32,\n    shuffle=True,\n    seed=42,\n    subset='training'\n)\n\nval_images = train_generator.flow_from_dataframe(\n    dataframe=train_df,\n    x_col='Filepath',\n    y_col='Label',\n    target_size=(224, 224),\n    color_mode='rgb',\n    class_mode='categorical',\n    batch_size=32,\n    shuffle=True,\n    seed=42,\n    subset='validation'\n)\n\ntest_images = test_generator.flow_from_dataframe(\n    dataframe=test_df,\n    x_col='Filepath',\n    y_col='Label',\n    target_size=(224, 224),\n    color_mode='rgb',\n    class_mode='categorical',\n    batch_size=32,\n    shuffle=False\n)","metadata":{"id":"pOKLejC8LACC","outputId":"80634b02-0719-4fbe-dc70-e95fb2c1cbb5","execution":{"iopub.status.busy":"2024-01-07T05:09:03.561777Z","iopub.execute_input":"2024-01-07T05:09:03.562136Z","iopub.status.idle":"2024-01-07T05:09:06.674438Z","shell.execute_reply.started":"2024-01-07T05:09:03.562100Z","shell.execute_reply":"2024-01-07T05:09:06.673233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Resize Layer\nresize_and_rescale = tf.keras.Sequential([\n  layers.experimental.preprocessing.Resizing(224,224),\n  layers.experimental.preprocessing.Rescaling(1./255),\n])\n\n# Setup data augmentation\ndata_augmentation = keras.Sequential([\n  preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n  preprocessing.RandomRotation(0.2),\n  preprocessing.RandomZoom(0.2),\n  preprocessing.RandomHeight(0.2),\n  preprocessing.RandomWidth(0.2),                       \n], name=\"data_augmentation\")","metadata":{"id":"RUcnQVa5LDbL","_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-01-07T05:09:06.675893Z","iopub.execute_input":"2024-01-07T05:09:06.676312Z","iopub.status.idle":"2024-01-07T05:09:06.700356Z","shell.execute_reply.started":"2024-01-07T05:09:06.676267Z","shell.execute_reply":"2024-01-07T05:09:06.699485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the pretained model\npretrained_model = tf.keras.applications.MobileNetV3Large(\n    input_shape=(224, 224, 3),\n    include_top=False,\n    weights='imagenet',\n    pooling='avg'\n)\n\npretrained_model.trainable = False","metadata":{"id":"kIF4DqQELGpc","outputId":"2bc0cb66-1ab3-445e-9aa2-1fe69825347a","execution":{"iopub.status.busy":"2024-01-07T05:09:06.701535Z","iopub.execute_input":"2024-01-07T05:09:06.701842Z","iopub.status.idle":"2024-01-07T05:09:08.156707Z","shell.execute_reply.started":"2024-01-07T05:09:06.701814Z","shell.execute_reply":"2024-01-07T05:09:08.155746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create checkpoint callback\ncheckpoint_path = \"yoga_classification_model_checkpoint\"\ncheckpoint_callback = ModelCheckpoint(checkpoint_path,\n                                      save_weights_only=True,\n                                      monitor=\"val_accuracy\",\n                                      save_best_only=True)","metadata":{"id":"6YxDprB_LJ7j","execution":{"iopub.status.busy":"2024-01-07T05:09:08.158366Z","iopub.execute_input":"2024-01-07T05:09:08.158810Z","iopub.status.idle":"2024-01-07T05:09:08.164729Z","shell.execute_reply.started":"2024-01-07T05:09:08.158765Z","shell.execute_reply":"2024-01-07T05:09:08.163088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Setup EarlyStopping callback to stop training if model's val_loss doesn't improve for 3 epochs\nearly_stopping = EarlyStopping(monitor=\"val_loss\", # watch the val loss metric\n                                                  patience=3,\n                                                  restore_best_weights=True) # if val loss decreases for 3 epochs in a row, stop training","metadata":{"id":"Zx8FP-ppLK8Y","execution":{"iopub.status.busy":"2024-01-07T05:09:08.166582Z","iopub.execute_input":"2024-01-07T05:09:08.167006Z","iopub.status.idle":"2024-01-07T05:09:08.179481Z","shell.execute_reply.started":"2024-01-07T05:09:08.166963Z","shell.execute_reply":"2024-01-07T05:09:08.178341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üöÑTrain the model","metadata":{"id":"1S9752FNOVl5"}},{"cell_type":"code","source":"inputs = pretrained_model.input\nx = resize_and_rescale(inputs)\nx = data_augmentation(x)\n\nx = Dense(256, activation='relu')(pretrained_model.output)\nx = Dropout(0.2)(x)\nx = Dense(256, activation='relu')(x)\nx = Dropout(0.2)(x)\n\noutputs = Dense(43, activation='softmax')(x)\n\nmodel = Model(inputs=inputs, outputs=outputs)\n\nmodel.compile(\n    optimizer=Adam(0.00001),\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\nhistory = model.fit(\n    train_images,\n    steps_per_epoch=len(train_images),\n    validation_data=val_images,\n    validation_steps=len(val_images),\n    epochs=100,\n    callbacks=[\n        early_stopping,\n        create_tensorboard_callback(\"training_logs\", \n                                    \"yoga_classification\"),\n        checkpoint_callback,\n    ]\n)","metadata":{"id":"cNF99dQTLL7s","outputId":"427cc983-0d58-4229-c51c-ca5fdfeccbed","_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-01-07T05:09:08.180960Z","iopub.execute_input":"2024-01-07T05:09:08.181308Z","iopub.status.idle":"2024-01-07T05:53:11.248850Z","shell.execute_reply.started":"2024-01-07T05:09:08.181277Z","shell.execute_reply":"2024-01-07T05:53:11.247856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = model.evaluate(test_images, verbose=0)\n\nprint(\"    Test Loss: {:.5f}\".format(results[0]))\nprint(\"Test Accuracy: {:.2f}%\".format(results[1] * 100))","metadata":{"id":"gAeFL4krM71Q","outputId":"64c2b696-5a8b-4871-8064-756483eb6b58","execution":{"iopub.status.busy":"2024-01-07T05:53:11.250507Z","iopub.execute_input":"2024-01-07T05:53:11.250876Z","iopub.status.idle":"2024-01-07T05:53:18.441956Z","shell.execute_reply.started":"2024-01-07T05:53:11.250842Z","shell.execute_reply":"2024-01-07T05:53:18.440839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üìâVisualizing loss curves","metadata":{"id":"2ygbh8KRM9-g"}},{"cell_type":"code","source":"plot_loss_curves(history)","metadata":{"id":"PhCey7FcNAwy","outputId":"cd417813-8988-4941-9f4d-ecbd3e6f1c1c","execution":{"iopub.status.busy":"2024-01-07T05:53:18.443527Z","iopub.execute_input":"2024-01-07T05:53:18.443977Z","iopub.status.idle":"2024-01-07T05:53:18.914660Z","shell.execute_reply.started":"2024-01-07T05:53:18.443935Z","shell.execute_reply":"2024-01-07T05:53:18.913677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üîÆMaking predictions on the test data","metadata":{"id":"EdpTHJLWNB1P"}},{"cell_type":"code","source":"# Predict the label of the test_images\npred = model.predict(test_images)\npred = np.argmax(pred,axis=1)\n\n# Map the label\nlabels = (train_images.class_indices)\nlabels = dict((v,k) for k,v in labels.items())\npred = [labels[k] for k in pred]\n\n# Display the result\nprint(f'The first 5 predictions: {pred[:5]}')","metadata":{"id":"BVxbYXCyNFdy","outputId":"8abed64f-6f21-4e01-c319-8358a80515e8","execution":{"iopub.status.busy":"2024-01-07T05:53:18.916439Z","iopub.execute_input":"2024-01-07T05:53:18.916878Z","iopub.status.idle":"2024-01-07T05:53:26.797583Z","shell.execute_reply.started":"2024-01-07T05:53:18.916836Z","shell.execute_reply":"2024-01-07T05:53:26.796460Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"  # Display 15 random pictures from the dataset with their labels\nrandom_index = np.random.randint(0, len(test_df) - 1, 15)\nfig, axes = plt.subplots(nrows=3, ncols=5, figsize=(25, 15),\n                        subplot_kw={'xticks': [], 'yticks': []})\n\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(plt.imread(test_df.Filepath.iloc[random_index[i]]))\n    if test_df.Label.iloc[random_index[i]] == pred[random_index[i]]:\n      color = \"green\"\n    else:\n      color = \"red\"\n    ax.set_title(f\"True: {test_df.Label.iloc[random_index[i]]}\\nPredicted: {pred[random_index[i]]}\", color=color)\nplt.show()\nplt.tight_layout()","metadata":{"id":"nOKEgIMKNGit","outputId":"f4645dfb-24f1-42b6-f7f6-f0dc513f62e6","execution":{"iopub.status.busy":"2024-01-07T05:53:26.798965Z","iopub.execute_input":"2024-01-07T05:53:26.799285Z","iopub.status.idle":"2024-01-07T05:53:28.884785Z","shell.execute_reply.started":"2024-01-07T05:53:26.799254Z","shell.execute_reply":"2024-01-07T05:53:28.883496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üìäPlotting the classification reports and confusion matrix","metadata":{"id":"E5jrrJk0NIrR"}},{"cell_type":"code","source":"y_test = list(test_df.Label)\nprint(classification_report(y_test, pred))","metadata":{"id":"-ufFysSCNmUc","outputId":"a191588c-f64d-4cd2-f70b-cdde9bd02404","execution":{"iopub.status.busy":"2024-01-07T05:53:28.886329Z","iopub.execute_input":"2024-01-07T05:53:28.886718Z","iopub.status.idle":"2024-01-07T05:53:28.924143Z","shell.execute_reply.started":"2024-01-07T05:53:28.886682Z","shell.execute_reply":"2024-01-07T05:53:28.922927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"report = classification_report(y_test, pred, output_dict=True)\ndf = pd.DataFrame(report).transpose()\ndf","metadata":{"id":"aZ7I3JDMNnhE","outputId":"7f96f0a5-1bc0-478b-b14e-e935f0ff2555","execution":{"iopub.status.busy":"2024-01-07T05:53:28.925802Z","iopub.execute_input":"2024-01-07T05:53:28.926185Z","iopub.status.idle":"2024-01-07T05:53:28.979481Z","shell.execute_reply.started":"2024-01-07T05:53:28.926150Z","shell.execute_reply":"2024-01-07T05:53:28.978297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_img_array(img_path, size):\n    img = tf.keras.preprocessing.image.load_img(img_path, target_size=size)\n    array = tf.keras.preprocessing.image.img_to_array(img)\n    # We add a dimension to transform our array into a \"batch\"\n    # of size \"size\"\n    array = np.expand_dims(array, axis=0)\n    return array\n\ndef make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n    # First, we create a model that maps the input image to the activations\n    # of the last conv layer as well as the output predictions\n    grad_model = tf.keras.models.Model(\n        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n    )\n\n    # Then, we compute the gradient of the top predicted class for our input image\n    # with respect to the activations of the last conv layer\n    with tf.GradientTape() as tape:\n        last_conv_layer_output, preds = grad_model(img_array)\n        if pred_index is None:\n            pred_index = tf.argmax(preds[0])\n        class_channel = preds[:, pred_index]\n    # This is the gradient of the output neuron (top predicted or chosen)\n    # with regard to the output feature map of the last conv layer\n    grads = tape.gradient(class_channel, last_conv_layer_output)\n\n    # This is a vector where each entry is the mean intensity of the gradient\n    # over a specific feature map channel\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n\n    # We multiply each channel in the feature map array\n    # by \"how important this channel is\" with regard to the top predicted class\n    # then sum all the channels to obtain the heatmap class activation\n    last_conv_layer_output = last_conv_layer_output[0]\n    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n    heatmap = tf.squeeze(heatmap)\n\n    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n    return heatmap.numpy()\ndef save_and_display_gradcam(img_path, heatmap, cam_path=\"cam.jpg\", alpha=0.4):\n    # Load the original image\n    img = tf.keras.preprocessing.image.load_img(img_path)\n    img = tf.keras.preprocessing.image.img_to_array(img)\n\n    # Rescale heatmap to a range 0-255\n    heatmap = np.uint8(255 * heatmap)\n\n    # Use jet colormap to colorize heatmap\n    jet = cm.get_cmap(\"jet\")\n\n    # Use RGB values of the colormap\n    jet_colors = jet(np.arange(256))[:, :3]\n    jet_heatmap = jet_colors[heatmap]\n\n    # Create an image with RGB colorized heatmap\n    jet_heatmap = tf.keras.preprocessing.image.array_to_img(jet_heatmap)\n    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n    jet_heatmap = tf.keras.preprocessing.image.img_to_array(jet_heatmap)\n\n    # Superimpose the heatmap on original image\n    superimposed_img = jet_heatmap * alpha + img\n    superimposed_img = tf.keras.preprocessing.image.array_to_img(superimposed_img)\n    # Save the superimposed image\n    superimposed_img.save(cam_path)\n\n    # Display Grad CAM\n#     display(Image(cam_path))\n    \n    return cam_path\n    \n\npreprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\ndecode_predictions = tf.keras.applications.mobilenet_v2.decode_predictions\n\nlast_conv_layer_name = \"Conv_1\"\nimg_size = (224,224, 3)\n\n# Remove last layer's softmax\nmodel.layers[-1].activation = None","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-07T06:20:55.897362Z","iopub.execute_input":"2024-01-07T06:20:55.898323Z","iopub.status.idle":"2024-01-07T06:20:55.917842Z","shell.execute_reply.started":"2024-01-07T06:20:55.898284Z","shell.execute_reply":"2024-01-07T06:20:55.916680Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display the part of the pictures used by the neural network to classify the pictures\nfig, axes = plt.subplots(nrows=3, ncols=5, figsize=(15, 10),\n                        subplot_kw={'xticks': [], 'yticks': []})\n\nfor i, ax in enumerate(axes.flat):\n    img_path = test_df.Filepath.iloc[random_index[i]]\n    img_array = preprocess_input(get_img_array(img_path, size=img_size))\n    heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name)\n    cam_path = save_and_display_gradcam(img_path, heatmap)\n    ax.imshow(plt.imread(cam_path))\n    ax.set_title(f\"True: {test_df.Label.iloc[random_index[i]]}\\nPredicted: {pred[random_index[i]]}\")\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-07T06:20:55.919860Z","iopub.execute_input":"2024-01-07T06:20:55.920220Z","iopub.status.idle":"2024-01-07T06:20:59.979491Z","shell.execute_reply.started":"2024-01-07T06:20:55.920189Z","shell.execute_reply":"2024-01-07T06:20:59.978503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom tensorflow.keras.models import load_model\n\n# Assuming 'val_images' contains validation data and labels\nval_data, val_labels = val_images\n\n# Load the trained model (replace 'model_path' with the actual path to your saved model)\nloaded_model = load_model('model_path')\n\n# Make predictions on the validation set\npredictions = loaded_model.predict(val_data)\n\n# Convert predictions and true labels to one-hot encoding\npredicted_labels = np.argmax(predictions, axis=1)\ntrue_labels = np.argmax(val_labels, axis=1)\n\n# Calculate evaluation metrics\naccuracy = accuracy_score(true_labels, predicted_labels)\nprecision = precision_score(true_labels, predicted_labels, average='weighted')\nrecall = recall_score(true_labels, predicted_labels, average='weighted')\nf1 = f1_score(true_labels, predicted_labels, average='weighted')\n\nprint(f'Accuracy: {accuracy:.4f}')\nprint(f'Precision: {precision:.4f}')\nprint(f'Recall: {recall:.4f}')\nprint(f'F1 Score: {f1:.4f}')\n","metadata":{"execution":{"iopub.status.busy":"2024-01-07T06:20:59.981054Z","iopub.execute_input":"2024-01-07T06:20:59.981920Z","iopub.status.idle":"2024-01-07T06:21:00.527638Z","shell.execute_reply.started":"2024-01-07T06:20:59.981869Z","shell.execute_reply":"2024-01-07T06:21:00.526375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom tensorflow.keras.models import load_model\n\n# Assuming 'val_images' is a tuple or list containing validation data and labels\nval_data, val_labels = val_images[0], val_images[1]\n\n# Load the trained model (replace 'model_path' with the actual path to your saved model)\n# loaded_model = load_model('model_path')\n\n# Make predictions on the validation set\npredictions = model.predict(val_data)\n\n# Convert predictions and true labels to one-hot encoding\npredicted_labels = np.argmax(predictions, axis=1)\ntrue_labels = np.argmax(val_labels, axis=1)\n\n# Calculate evaluation metrics\naccuracy = accuracy_score(true_labels, predicted_labels)\nprecision = precision_score(true_labels, predicted_labels, average='weighted')\nrecall = recall_score(true_labels, predicted_labels, average='weighted')\nf1 = f1_score(true_labels, predicted_labels, average='weighted')\n\nprint(f'Accuracy: {accuracy:.4f}')\nprint(f'Precision: {precision:.4f}')\nprint(f'Recall: {recall:.4f}')\nprint(f'F1 Score: {f1:.4f}')\n","metadata":{"execution":{"iopub.status.busy":"2024-01-07T06:22:12.649526Z","iopub.execute_input":"2024-01-07T06:22:12.650475Z","iopub.status.idle":"2024-01-07T06:22:13.197545Z","shell.execute_reply.started":"2024-01-07T06:22:12.650435Z","shell.execute_reply":"2024-01-07T06:22:13.195708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}